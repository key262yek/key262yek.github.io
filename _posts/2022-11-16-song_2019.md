---
title: "Paper Review : Generative Modeling by Estimating Gradients of the Data Distribution"
date: 2022-11-16
categories:
  - Review
  - Programming
tags:
  - Langevin equation
  - Generative Modeling
  - Diffusion model
published: true
toc: true
toc_label: "Table of Contents"
toc_icon: "fas fa-clipboard-list"
toc_sticky: true
---

## Paper information
Yang Song, and Stefano Ermon, 
"Generative Modeling by Estimating Gradients of the Data Distribution",
[NeurIPS **32** (2019)](https://arxiv.org/abs/1907.05600)

## Abstract
We introduce a new generative model where samples are produced via Langevin dynamics using gradients of the data distribution estimated with score matching. Because gradients can be ill-defined and hard to estimate when the data resides on low-dimensional manifolds, we perturb the data with different levels of Gaussian noise, and jointly estimate the corresponding scores, i.e., the vector fields of gradients of the perturbed data distribution for all noise levels. For sampling, we propose an annealed Langevin dynamics where we use gradients corresponding to gradually decreasing noise levels as the sampling process gets closer to the data manifold. Our framework allows flexible model architectures, requires no sampling during training or the use of adversarial methods, and provides a learning objective that can be used for principled model comparisons. Our models produce samples comparable to GANs on MNIST, CelebA and CIFAR-10 datasets, achieving a new state-of-the-art inception score of 8.87 on CIFAR-10. Additionally, we demonstrate that our models learn effective representations via image inpainting experiments.

## Limitation of precedent research
Likelihood-based methods의 한계점
- 문제마다 특징적인 구조를 사용해야만 함
- Surrogate loss를 사용해야 함
  
Generative adversarial networks
- 학습이 불안정해 끝나지 않을 수 있음
- 다른 모델과의 직접 비교가 쉽지 않음

그래서 data point에서의 log-density의 gradient를 score로 삼아 학습하는 새로운 방법론을 제시하려 한다. 
