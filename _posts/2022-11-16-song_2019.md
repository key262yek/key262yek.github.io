---
title: "Paper Review : Generative Modeling by Estimating Gradients of the Data Distribution"
date: 2022-11-16
categories:
  - Review
  - Programming
tags:
  - Langevin equation
  - Generative Modeling
  - Diffusion model
published: true
toc: true
toc_label: "Table of Contents"
toc_icon: "fas fa-clipboard-list"
toc_sticky: true
---

## Paper information
Yang Song, and Stefano Ermon, 
"Generative Modeling by Estimating Gradients of the Data Distribution",
[NeurIPS **32** (2019)](https://arxiv.org/abs/1907.05600)

## Abstract
We introduce a new generative model where samples are produced via Langevin dynamics using gradients of the data distribution estimated with score matching. Because gradients can be ill-defined and hard to estimate when the data resides on low-dimensional manifolds, we perturb the data with different levels of Gaussian noise, and jointly estimate the corresponding scores, i.e., the vector fields of gradients of the perturbed data distribution for all noise levels. For sampling, we propose an annealed Langevin dynamics where we use gradients corresponding to gradually decreasing noise levels as the sampling process gets closer to the data manifold. Our framework allows flexible model architectures, requires no sampling during training or the use of adversarial methods, and provides a learning objective that can be used for principled model comparisons. Our models produce samples comparable to GANs on MNIST, CelebA and CIFAR-10 datasets, achieving a new state-of-the-art inception score of 8.87 on CIFAR-10. Additionally, we demonstrate that our models learn effective representations via image inpainting experiments.

## Introduction to Generative modeling of image
이 논문에서는 이미지를 생성하는 Score-based generative modeling(SBGM)에 대해 설명하고 있습니다. 
논문 내용에 본격적으로 들어가기 앞서 Generative modeling에 대해 간략히 알아보고 갑시다.
### What is `image` and `image-like object`?
먼저, 만들고자 하는 이미지란 무엇을 말하는걸까요?
컴퓨터에게 이미지란 pixel마다 rgb 값이 부여된 2D 행렬입니다. 
그러면 모든 rgb 행렬은 이미지일까요?

Random noise              |  Real Image
:-------------------------:|:-------------------------:
![noise_image](https://key262yek.github.io/assets/images/noise_image.png)  |  ![real_image](https://key262yek.github.io/assets/images/real_image_cifar-10.jpg)

우리는 위의 그림들 중 오른쪽만을 이미지라고 합니다. 
pixel간의 상관관계가 전혀 없이 무작위적으로 rgb 값이 부여된 행렬은 이미지라고 보기 어렵죠.
사진 상의 객체는 보통 여러개의 pixel에 걸쳐 표시되기 때문에, 이들은 색 정보적으로 연속적일 필요가 있습니다.
이러한 제한 조건 탓에 rgb 행렬 중 사진으로 불릴 수 있는 경우는 매우 극소수에 불과할 것이며, 
심지어는 그 차원도 (전체 공간의 차원에 비해) 작을 것입니다. 

그렇다면 우리가 만들고자 하는 `사진`은 어떻게 이해하면 좋을까요?
사람은 눈을 통해 pixel들의 색 정보를 총체적으로 받아들이면서 사진 속 객체들을 찾아내고, 그 과정에서 이를 사진으로 인식하게 됩니다. 
하지만 컴퓨터에게 객체를 인식하도록 하는 것은 쉬운 문제가 아니죠. 
그래서 기계학습에서는 컴퓨터에게 사진을 많이 보여줌으로써 `사진 같은 것`이 무엇인지 학습할 수 있도록 합니다. 
주어진 사진 데이터의 주변도 사진일 것이라는 가정 속에서, 전체 공간에서 `사진 같은 것`들이 존재하는 공간이 어디인지 배우는 것이죠. 

### Generative modeling
좀 더 수학적으로 따지자면 이렇습니다.
$N \times N$개 pixel의 RGB 정보를 담은 벡터 $\vb{x}$는 $[0, 1]^{3 N^2}$ 공간의 원소일 것입니다.
($[0, 1]$은 normalized 된 rgb 정보의 범위이고, $3N^2$은 벡터의 성분 숫자입니다.)
이 공간에서 `사진 같은 것`들의 분포를 나타내는 함수 $p(\vb{x})$를 알아내는 것이 기계학습의 최종 목적입니다.

물론 사진 공간은 매우 넓고 그에 비해 데이터는 매우 부족하기 때문에 기계학습이 학습한 데이터 분포 $p_{data}(\vb{x})$는 실제 사진의 분포 $p(\vb{x})$와 다를 수 있습니다.
하지만 때때로 우리는 모든 사진 중 하나를 원하는 것이 아니라, 특징적인 이미지를 원하는 경우가 있습니다.
MNIST의 필기체 사진이나, 많이들 이용하는 얼굴 사진 같은 경우가 그 예시죠. 
전체 사진공간 전체가 아니라, `필기체 사진`의 분포를 보거나, `얼굴 사진`의 분포를 알아내는 것이 중요한 경우가 있을 수 있고, 그런 경우 이러한 제한적인 분포를 학습하는 것은 전혀 문제가 되지 않습니다.

이렇게 학습한 $p_{data}(\vb{x})$를 이용해 이 분포를 따르는 random number를 만드는 generator가 바로 우리가 보게되는 Generative model들입니다.
그렇다면 이 논문에서는 어떤 방법을 통해 random number를 만들까요?

### Langevin dynamics and Random number generation
임의의 분포를 따르는 random number generation에는 여러 방법이 있습니다.
Markov chain Monte Carlo method의 일환인 Metropolis-Hasting algorithm이란 방법이 있고,
여기서 사용하는 Langevin dynamics를 이용하는 방법이 있습니다.

![langevin](https://key262yek.github.io/assets/images/Langevin_dynamics.jpg)

Langevin dynamics는 `무작위적인 힘`이 가해지는 입자의 운동을 지칭합니다.
이 무작위적인 힘은 다양한 원천이 있습니다. 
유체 속에 있는 입자의 경우에는 유체 분자의 무작위적 충돌의 총합이,
사람의 의사결정의 변화를 따질 때에는 외부에서 제공되는 여러 정보들을 무작위적 충격(kick)으로 봅니다. 
이러한 무작위적 힘을 이용해 입자의 운동을 예측하는 영역이 Langevin dynamics입니다.

Newtonian dynamics에서 Newton 방정식이 있듯이, Langevin dynamics에도 Langevin 방정식이 있는데요.
그 일반적인 형태는 아래와 같습니다. 
\begin{equation}
  m \dv{v}{t} = - \gamma v + F + \xi(t)
\end{equation}
물체의 가속도가 총 세 개의 힘에 의해 결정된다는 방정식인데, 
첫번째부터 순서대로 유체로부터 받는 저항력, 외부로부터 받는 힘, 유체로부터 받은 무작위적 충격을 나타냅니다.
이러한 방정식을 underdamped Langevin equation이라 하기도 합니다. 

이때 입자의 속도는 유체로부터 받는 저항력에 의하여 특정 속도로 수렴하는 경향성을 가집니다. 
만약 저항력이 매우 세서 물체의 속도가 매우 짧은 시간안에 수렴해버리는 경우를 우리는 overdamped system이라 합니다.
속도가 수렴하는 시간을 무시해버릴 수 있기 때문에 입자의 속도는 매 순간 외력에 의해 정해지고, 가속도는 곧바로 0이 되는 상황($\dv{v}{t} = 0$)을 생각할 수 있습니다. 
앞선 underdamped Langevin equation에 이 조건을 대입하면 아래의 식이 나오는데, 이를 두고 overdamped Langevin equation이라 합니다.
\begin{equation}
  v = \frac{1}{\gamma} F + \frac{\xi(t)}{\gamma}
\end{equation}

여기서 만약 외력 $F$가 어떤 위치에너지 $V(x)$에 의해 결정된다면, 힘과 위치에너지 사이 관계 $F = - \dv{V}{x}$를 이용해 식이 한 차례 더 바뀌게 됩니다.
\begin{equation}
  v = - \frac{1}{\gamma} \dv{V}{x} + \frac{\xi(t)}{\gamma}
\end{equation}
이러한 방정식을 만족하며 움직이는 입자의 위치는 (무작위성에 의해) 시간에 따라 달라지는 어떠한 분포를 통해 결정되는데요.
만약 시간이 충분히 지나 해당 분포가 `평형 상태`에 놓이게 되면 그 분포는 필연적으로 위치에너지 $V(x)$에 의해 결정되는 `볼츠만 분포`를 따르게 됩니다.
\begin{equation}
  P(x) \propto \exp(- \frac{V(x)}{k_B T})
\end{equation}
여기서 $k_B T$는 시스템의 온도를 의미하며, `Einstein relation`에 따라 저항계수 $\gamma$와 확산 계수 $D$에 대하여 $k_B T = \gamma D$를 만족합니다.
즉, 온도가 오르면 그만큼 확산이 위치에너지를 무시할 정도로 잘 일어나서 위치에너지와 무관하게 균등한 분포가 만들어지고,
온도가 낮아지면 확산이 줄어들어 위치에너지에 강하게 상관관계를 가지는 분포가 만들어지는 것입니다. 

즉, 온도 $T$와 위치에너지 $V(x)$를 잘 조절하면 원하는 평형 분포를 유도할 수 있게 되는 것입니다. 
만약 어떤 확률분포 $p(x)$에 대하여 위치에너지를 $V(x) = - k_B T log p(x)$로 정하게 된다면, 긴 시간 움직인 입자들의 평형 분포가 바로 $p(x)$가 되는 것이죠.
이것이 해당 논문에서 데이터 분포를 따르는 random number를 만드는 방식이자, 힘의 역할을 하는 score가 $\nabla_x log p(x)$로 주어지는 이유라 할 수 있습니다. 


## Characteristics of Score-based generative modeling
개괄은 끝났습니다. Generative modeling은 다름이 아니라 주어진 데이터로 학습한 확률분포에 따라 random number를 뱉어내는 generator입니다.
이제 다음으로 넘어가 해당 논문에서 소개하는 Score-based modeling에 대한 이야기를 해봅시다. 
기본적으로 이 모델은 데이터 분포의 `기울기`를 학습하고, 이 기울기를 힘(score)로 삼는 Langevin dynamics를 거쳐 이미지를 생성하는 모델입니다.
매우 단순한 구조인데 이것만으로는 좋은 성능을 기대하기 어려운데, 그 이유에 대해 살펴봅니다.
### Main challenges on Training data distribution
데이터 분포를 학습하는 과정에는 성능에 크게 영향을 미치는 두 가지 문제가 있습니다.
#### Low-dimensionality of data
앞서 이야기했듯이 무작위적인 색의 배열은 사진이 아닙니다.
수많은 제한 조건을 만족하는 색 배열만이 사진으로 인정받을 수 있죠. 
이는 색 배열의 전체 공간 중에서 사진으로 인식되는 영역이 단순히 비율이 작은 것을 넘어 작은 차원의 공간이 될 것임을 시사합니다.
3차원 공간 속에서 평면이나 선의 비율은 0이듯이, 전체 공간에서 사진으로 인식되는 영역 역시 그 비율이 0이 되는 것이죠.
즉 색 배열 전체 공간에서 거의 모든 영역은 사진의 확률분포가 0인 점들이란 것이고, 따라서 그 기울기도 0이 되어 Langevin dynamics를 통해 사진과 비슷한 영역으로 유도할 수 없게 됩니다.

이를 해결하기 위한 방법이 바로 사진에 noise를 넣는 방식입니다. 
noise는 주변 pixel과의 상관관계가 전혀 없기 때문에 `사진이기 위한 제한조건`과 무관하게 데이터 점들을 이동시킬 수 있고, 그 결과 사진에 noise를 끼운 `사진 같은 것`들의 영역이 부피를 가지도록 할 수 있습니다.

Random noise              |  Real Image
:-------------------------:|:-------------------------:
{% include plots/real_data.html %}  |  {% include plots/noisy_data.html %}



- Training dataset은 그보다 더 제한적일 것이다. 이는 gradient의 정확성을 떨어뜨리고, 높지 않은 density 영역을 돌아다니게 만들어 sample 생성과정을 느리게 만들 것이다. 

이러한 문제를 해결하기 위해 데이터에 **Gaussian noise**를 추가하는 방식을 제안한다. 
noise는 더이상 low-dimensional 하지 않을 것이기 때문에, noise가 가해진 data domain은 아래 그림과 같이 전 영역으로 넓게 퍼질 수 있게 된다. 

![dimension](https://key262yek.github.io/assets/images/random_noise_low_dimension.jpg)

아래 그림을 보면 random noise가 없이 학습한 경우 학습이 거의 진행되지 않고 loss가 오르내리는 것을 볼 수 있지만,
random noise를 추가한 경우엔 순조롭게 학습이 진행되는 것을 확인할 수 있다. 

![stability](https://key262yek.github.io/assets/images/random_noise_learning_stability.PNG)

random noise는 low-density 영역으로 데이터를 옮겨가게 만듦으로써, gradient를 만들어내는 역할을 할 수 있다. 

![noise](https://key262yek.github.io/assets/images/annealed_random_noise.jpg)

하지만 random noise가 너무 크면 입자들의 최종 분포가 원하는 데이터 분포와 달라질 수 있으므로, 
noise level을 변화시키면서 gradient를 학습시켰다. (하지만 single score network임)
처음엔 강한 noise에서 sample이 움직여 low-density 영역을 탈출할 수 있도록 하고, 점차 약한 noise로 바꾸어 원하는 데이터 분포를 따르도록 유도하는 방식을 쓸 수 있다. 
(이는 simulated annealing[^1][^2]에서 영감을 받았다.)

이런 현상은 아래 그림에서 확인할 수 있다. 
가중치가 다른 두 분포가 섞여있을 때, 이 합성 분포의 gradient를 그대로 이용해 움직이는 입자는 low-density 영역을 넘어가지 못해 가중치를 적절히 반영하지 못합니다. 
하지만 annealed Langevin dynamics를 적용한 경우에는 원래 sample을 그대로 복원한 것을 볼 수 있습니다. 

![annealed_langevin](https://key262yek.github.io/assets/images/annealed_langevin_experiment.PNG)

[^1]: S.Kirkpatric et al. "Optimiation by simulated annealing". *Science* (1983)
[^2]: R.M. Neal. "Annealed importance sampling". *Statistics and computing*. (2001)

## Properties
- Tractable
- Optimized without adversarial training, MCMC sampling etc.  
- Available to quantitatively comapare different models by using efficacy. 

## Networks
### Noise conditional score networks
위치와 noise level을 같이 input 했을 때의 distribution gradient를 학습하는 network

특징적인 network 구조는 사용하지 않음.
Image generation을 예시로 들었을 때, image의 pixel 정보를 모두 받아 같은 차원의 gradient vector를 만들기 때문에 이는 semantic segmentation과 그 구조가 같음.  
해당 분야에서 유명한 U-Net [^3]과 dilated/atrous convolution [^4]

[^3]: O.Ronneberger et al. "U-Net: Convolutional networks for biomedical image segmentation". *MICCAI*, (2015)
[^4]: F. Yu et al. "Multi-scale context aggregation by dilated convolutions". *ICLR*. (2016)

![semantic_segmentation](https://key262yek.github.io/assets/images/semantic_segmentation.PNG)

Instance normalization [^5]

[^5]: D. Ulyanov et al. "Instance normalization: The missing ingredient for fast stylization". arXiv (2016)

## Langevin dynamics
### simulate Langevin dynamics

이를 매우 짧은 시간간격 $\dd t$에 대해 적분하면 짧은 시간 동안의 물체의 변위를 구할 수 있습니다.
\begin{equation}
  x(t + \dd t) = x(t) + \frac{1}{\gamma} F \dd t + \sqrt{2 D \dd t} z
\end{equation}
여기서 $z$는 normalized Gaussian noise이고, $D$는 입자의 확산계수라고 불리는 물리량입니다. 
무작위적 충격 $\xi$의 시간 적분이 왜 Gaussian noise의 형태로 나타나고, 확산 계수 $D$가 어떤 의미를 가지는지는 
각각 `Central limit theorem`과 `Fluctuation-Dissipation theorem`을 이용해 설명할 수 있습니다. 

### Constant signal-to-noise ratio

## 다른 모델과의 차이점
GANs - adversarial network가 필요함

Sohl. Dickstein은 Noisy distribution에서 data distribution을 복원하는 kernel as Markov chain을 학습하는 방식.
data distribution에서 random noise 가는 과정을 학습할 때 매우 조금씩 변화시켜야 하며, 그로 인해 수천번의 학습이 필요함. 

이 논문에서는 noisy picture에서 Random motion을 거쳐 data distribution 위의 한 점으로 이동하게 하는 method를 학습한 것. 
adversarial network도 필요 없음. 


## Results
![result](https://key262yek.github.io/assets/images/song_2019_result.PNG)





