---
title: "Paper Review : Generative Modeling by Estimating Gradients of the Data Distribution"
date: 2022-11-16
categories:
  - Review
  - Programming
tags:
  - Langevin equation
  - Generative Modeling
  - Diffusion model
published: true
toc: true
toc_label: "Table of Contents"
toc_icon: "fas fa-clipboard-list"
toc_sticky: true
---

## Paper information
Yang Song, and Stefano Ermon, 
"Generative Modeling by Estimating Gradients of the Data Distribution",
[NeurIPS **32** (2019)](https://arxiv.org/abs/1907.05600)

## Abstract
We introduce a new generative model where samples are produced via Langevin dynamics using gradients of the data distribution estimated with score matching. Because gradients can be ill-defined and hard to estimate when the data resides on low-dimensional manifolds, we perturb the data with different levels of Gaussian noise, and jointly estimate the corresponding scores, i.e., the vector fields of gradients of the perturbed data distribution for all noise levels. For sampling, we propose an annealed Langevin dynamics where we use gradients corresponding to gradually decreasing noise levels as the sampling process gets closer to the data manifold. Our framework allows flexible model architectures, requires no sampling during training or the use of adversarial methods, and provides a learning objective that can be used for principled model comparisons. Our models produce samples comparable to GANs on MNIST, CelebA and CIFAR-10 datasets, achieving a new state-of-the-art inception score of 8.87 on CIFAR-10. Additionally, we demonstrate that our models learn effective representations via image inpainting experiments.

## Limitation of precedent research
Likelihood-based methods의 한계점
- 문제마다 특징적인 구조를 사용해야만 함
- Surrogate loss를 사용해야 함
  
Generative adversarial networks
- 학습이 불안정해 끝나지 않을 수 있음
- 다른 모델과의 직접 비교가 쉽지 않음

그래서 data point에서의 log-density의 gradient를 학습하는 방법론을 제시하려 한다.
이 gradient는 이후 sample generation 단계에서 sample이 most-probable한 지점으로 이동하도록 하는 단서가 된다. 

## Main challenges and Tackles
Probability density를 학습하는 방법에는 크게 두 가지 문제가 있다.
- Data distribution은 가능한 공간에 비해 매우 제한적이고, low-dimensional할 것이다. 이 경우 gradient가 정의되지 않는 영역이 매우 많아 이 영역에서 시작하는 sample 생성이 이뤄질 수 없다. 
- Training dataset은 그보다 더 제한적일 것이다. 이는 gradient의 정확성을 떨어뜨리고, 높지 않은 density 영역을 돌아다니게 만들어 sample 생성과정을 느리게 만들 것이다. 

이러한 문제를 해결하기 위해 데이터에 **Gaussian noise**를 추가하는 방식을 제안한다. 
noise는 더이상 low-dimensional 하지 않을 것이기 때문에 첫 번째 문제를 해결해줄 수 있다. 
그리고 noise level을 변화시키면서 gradient를 학습시켰는데 (하지만 single score network임), 이를 이용해 처음엔 강한 noise에서 sample이 움직이도록 하다가 점차 약한 noise로 바꾸어 initial data distribution의 영역으로 유도하는 방식을 쓸 수 있다. (annealed Langevin dynamics)
(이는 simulated annealing[^1][^2]에서 영감을 받았다.)

[^1]: S.Kirkpatric et al. "Optimiation by simulated annealing". *Science* (1983)
[^2]: R.M. Neal. "Annealed importance sampling". *Statistics and computing*. (2001)

## Properties
- Tractable
- Optimized without adversarial training, MCMC sampling etc. 
- Available to quantitatively comapare different models by using efficacy. 

## Networks
### Noise conditional score networks
위치와 noise level을 같이 input 했을 때의 distribution gradient를 학습하는 network

특징적인 network 구조는 사용하지 않음.
Image generation을 예시로 들었을 때, image의 pixel 정보를 모두 받아 같은 차원의 gradient vector를 만들기 때문에 이는 semantic segmentation과 그 구조가 같음.  
해당 분야에서 유명한 U-Net [^3]과 dilated/atrous convolution [^4]

[^3]: O.Ronneberger et al. "U-Net: Convolutional networks for biomedical image segmentation". *MICCAI*, (2015)
[^4]: F. Yu et al. "Multi-scale context aggregation by dilated convolutions". *ICLR*. (2016)

(Semantic Segmentation 예시)

Instance normalization [^5]
[^5]: D. Ulyanov et al. "Instance normalization: The missing ingredient for fast stylization". arXiv (2016)

## Langevin dynamics
Basic
constant signal-to-noise ratio

## 다른 모델과의 차이점
GANs - adversarial network가 필요함

Sohl. Dickstein은 Noisy distribution에서 data distribution을 복원하는 kernel as Markov chain을 학습하는 방식.
data distribution에서 random noise 가는 과정을 학습할 때 매우 조금씩 변화시켜야 하며, 그로 인해 수천번의 학습이 필요함. 

이 논문에서는 noisy picture에서 Random motion을 거쳐 data distribution 위의 한 점으로 이동하게 하는 method를 학습한 것. 
adversarial network도 필요 없음. 









